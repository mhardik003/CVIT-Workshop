{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using wandb to track experiments.\n",
        "\n",
        "Demo task: multi-class image classification using CIFAR10 dataset."
      ],
      "metadata": {
        "id": "h6hjehhHW4tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "OUnLA0ASMK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The next cell includes-\n",
        "- Collecting the CIFAR10 dataset and defining data loaders.\n",
        "- Methods to load model, criterion, optimizer and schedulers.\n",
        "- Definition of AverageMeter"
      ],
      "metadata": {
        "id": "s8hEreACXETi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading CIFAR10 dataset\n",
        "inp_transforms = T.Compose([T.ToTensor(),\n",
        "                            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])])\n",
        "tgt_transforms = T.Lambda(lambda y: torch.zeros(10, dtype=torch.long).scatter_(0, torch.tensor(y), value=1))\n",
        "cifar10 = datasets.CIFAR10(root = \"/.\",\n",
        "                           transform = inp_transforms,\n",
        "                           target_transform = tgt_transforms,\n",
        "                           download = True)\n",
        "\n",
        "# Defining dataset split (80-20)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(cifar10,\n",
        "                                                           [int(len(cifar10)*0.80), int(len(cifar10)*0.20)])\n",
        "\n",
        "# Defining the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=200,\n",
        "                              shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=200,\n",
        "                            shuffle=False)\n",
        "\n",
        "\n",
        "# Method to get model based on config param model_type\n",
        "def get_model(model_type):\n",
        "    model = None\n",
        "    if model_type == \"pretrained\": # Loading pretrained ResNet18 and with updated to final fc layer. \n",
        "        model = models.resnet18(pretrained=True)\n",
        "        model.fc = nn.Linear(512, 10)\n",
        "        model = model.to(device)\n",
        "    elif model_type == \"scratch\": # Loading a blank ResNet18 which generated 10 outputs.\n",
        "        model = models.resnet18(num_classes=10)\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        raise NotImplemented\n",
        "    return model\n",
        "\n",
        "\n",
        "# Method to get criterion, optimizer and scheduler based on config params.\n",
        "def get_criterion_optimizer_scheduler(config, model):\n",
        "    optim_dct = {\n",
        "        \"adam\": optim.Adam,\n",
        "        \"SGD\": optim.SGD,\n",
        "        \"RMSprop\": optim.RMSprop\n",
        "    }\n",
        "    optimizer = optim_dct[config[\"optimizer\"]](model.parameters(), lr=config[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           factor=0.1,\n",
        "                                                           patience=config[\"scheduler_patience\"],\n",
        "                                                           threshold=config[\"scheduler_thresh\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "\n",
        "\n",
        "# Remainder of this cell includes definition of AverageMeter (can be ignored)\n",
        "\"\"\"\n",
        "Code taken from Pytorch ImageNet examples\n",
        "https://github.com/pytorch/examples/blob/main/imagenet/main.py#L375\n",
        "\"\"\"\n",
        "class Summary():\n",
        "    NONE = 0\n",
        "    AVERAGE = 1\n",
        "    SUM = 2\n",
        "    COUNT = 3\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.summary_type = summary_type\n",
        "        self.val_history = list()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.val_history = list()\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.val_history.append(val)\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "    \n",
        "    def summary(self):\n",
        "        fmtstr = ''\n",
        "        if self.summary_type is Summary.NONE:\n",
        "            fmtstr = ''\n",
        "        elif self.summary_type is Summary.AVERAGE:\n",
        "            fmtstr = '{name} {avg:.3f}'\n",
        "        elif self.summary_type is Summary.SUM:\n",
        "            fmtstr = '{name} {sum:.3f}'\n",
        "        elif self.summary_type is Summary.COUNT:\n",
        "            fmtstr = '{name} {count:.3f}'\n",
        "        else:\n",
        "            raise ValueError('invalid summary type %r' % self.summary_type)        \n",
        "        return fmtstr.format(**self.__dict__)\n"
      ],
      "metadata": {
        "id": "N0IyFq2jMUTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Following cell includes-\n",
        "- Defining the train and eval loops.\n",
        "- Method to trigger training loops based on config parameters."
      ],
      "metadata": {
        "id": "UIEngQlXXfi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The train function without wandb logging\n",
        "\n",
        "def train(model, criterion, optimizer, scheduler, epochs, train_dataloader, val_dataloader, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loss_meter = AverageMeter(\"train_loss\", \":.5f\")\n",
        "        epoch_outs, epoch_tgt = list(), list()\n",
        "        for data, tgt_vec in tqdm(train_dataloader):\n",
        "            data, tgt_vec = data.to(device), tgt_vec.to(device)\n",
        "            targets = torch.argmax(tgt_vec, axis=1)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, targets)\n",
        "            loss_meter.update(loss.item(), data.shape[0])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_outs.append(out)\n",
        "            epoch_tgt.append(tgt_vec)\n",
        "        predictions = torch.vstack([torch.softmax(out, axis=1) for out in epoch_outs]).detach().cpu().numpy()\n",
        "        targets = torch.cat([tgt for tgt in epoch_tgt], dim=0).detach().cpu().numpy()\n",
        "        ap_score = average_precision_score(targets, predictions)\n",
        "        eval_loss_meter, eval_ap_score = evaluate(model, criterion, val_dataloader, device)\n",
        "        data_to_log = {\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": loss_meter.avg,\n",
        "            \"eval_loss\": eval_loss_meter.avg,\n",
        "            \"train_ap_score\": ap_score,\n",
        "            \"eval_ap_score\": eval_ap_score,\n",
        "            \"lr\": optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
        "        }\n",
        "        scheduler.step(eval_loss_meter.avg)\n",
        "        print(data_to_log)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, val_dataloader, device):\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter(\"eval_loss\", \":.5f\")\n",
        "    epoch_outs, epoch_tgt = list(), list()\n",
        "    for data, tgt_vec in val_dataloader:\n",
        "        data, tgt_vec = data.to(device), tgt_vec.to(device)\n",
        "        targets = torch.argmax(tgt_vec, axis=1)\n",
        "        out = model(data)\n",
        "        loss = criterion(out, targets)\n",
        "        loss_meter.update(loss.item(), data.shape[0])\n",
        "        epoch_outs.append(out)\n",
        "        epoch_tgt.append(tgt_vec)\n",
        "    predictions = torch.vstack([torch.softmax(out, axis=1) for out in epoch_outs]).detach().cpu().numpy()\n",
        "    targets = torch.cat([tgt for tgt in epoch_tgt], dim=0).detach().cpu().numpy()\n",
        "    ap_score = average_precision_score(targets, predictions)\n",
        "    return loss_meter, ap_score\n",
        "\n",
        "\n",
        "def trigger_training(config):\n",
        "    model = get_model(config[\"model_type\"])\n",
        "    criterion, optimizer, scheduler = get_criterion_optimizer_scheduler(config, model)\n",
        "    epochs = config[\"num_epochs\"]\n",
        "\n",
        "    train(model, criterion, optimizer, scheduler, epochs, train_dataloader, val_dataloader, device)\n"
      ],
      "metadata": {
        "id": "jfchRAmxMcoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete the config file, edit the cells in this notebook to log data to wandb and trigger training loops!"
      ],
      "metadata": {
        "id": "Jtb0ll98YTB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the Config file below and log the experiment at wandb\n",
        "config = {\n",
        "    \"lr\": 0.0, \n",
        "    \"model_type\": \"\", # pretrained/scratch\n",
        "    \"optimizer\": \"\", # adam/SGD/RMSprop\n",
        "    \"criterion\": \"ce\",\n",
        "    \"scheduler_patience\": 3,\n",
        "    \"scheduler_thresh\": 0.001,\n",
        "    \"num_epochs\": 0, # CHANGE\n",
        "    \"gpu_id\": 0,\n",
        "    \"wandb_run_name\": \"\" ### FILL YOUR NAME HERE\n",
        "}\n"
      ],
      "metadata": {
        "id": "2XWOBnxhMkdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigger_training(config)\n"
      ],
      "metadata": {
        "id": "BLGywkQ4NDMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyGt96kZN60-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RfnKkXnN7Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XYU1R7SN7at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1u1MT1lHN7id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WandB Steps"
      ],
      "metadata": {
        "id": "4br2c7d1N8ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Import WandB in your code\n",
        "\n",
        "import wandb\n",
        "\n",
        "### Step 1 ends"
      ],
      "metadata": {
        "id": "gg1VnOOBOAGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2:\n",
        "# Initiate wandb in your script. The moment we trigger wandb.init(), an active\n",
        "# socket connection is established between your machine and wandb server.\n",
        "# We specify the entity (wandb username) and project (which wandb project to use for logging)\n",
        "\n",
        "wandb.init(entity = \"dhruv_sri\",   # wandb username. (NOT REQUIRED ARG. ANYMORE, it fetches from initial login)\n",
        "           project = \"wandb_demo\", # wandb project name. New project will be created if given project is missing.\n",
        "           config = config         # Config dict\n",
        "          )\n",
        "wandb.run.name = config[\"wandb_run_name\"]\n",
        "\n",
        "### Step 2 ends.\n"
      ],
      "metadata": {
        "id": "2oIAE7tpOBH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Trigger wandb log\n",
        "# This step is responsible for sending the logs to wandb\n",
        "\n",
        "wandb.log(data_to_log)\n",
        "\n",
        "### Step 3 ends.\n"
      ],
      "metadata": {
        "id": "P_c8_juPOC6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4 (Optional)\n",
        "# This closes the active socket connection to wandb server. Optional since wandb destructor does the same.\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "### Step 4 ends.\n"
      ],
      "metadata": {
        "id": "JnQzf5xHOEKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fQ5XateZfx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GY131mhVZfvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WandB sweeps related steps"
      ],
      "metadata": {
        "id": "UiUK11qqZQZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1:\n",
        "# Create a WandB sweep config file.\n",
        "# This config file will be used at the WandB website to initialize a sweep server\n",
        "program: \"demo.py\"\n",
        "method: \"grid\"\n",
        "metric:\n",
        "  name: \"eval_ap_score\"\n",
        "  goal: \"maximize\"\n",
        "parameters:\n",
        "    criterion:\n",
        "      value: \"ce\"\n",
        "    gpu_id:\n",
        "      value: 0\n",
        "    lr:\n",
        "      values: [0.1, 0.001, 0.0001]\n",
        "    model_type:\n",
        "      values: [\"scratch\", \"pretrained\"]\n",
        "    num_epochs:\n",
        "      value: 25\n",
        "    optimizer:\n",
        "      values: [\"adam\", \"SGD\", \"RMSprop\"]\n",
        "    scheduler_patience:\n",
        "      value: 3\n",
        "    scheduler_thresh:\n",
        "      value: 0.01\n",
        "\n",
        "        \n",
        "### A sample sweep config file if bayes method is used-\n",
        "# program: wandb_demo.py\n",
        "# method: bayes\n",
        "# metric:\n",
        "#   name: \"eval_ap_score\"\n",
        "#   goal: maximize\n",
        "# parameters:\n",
        "#   lr:\n",
        "#     distribution: uniform\n",
        "#     min: 0.00001\n",
        "#     max: 0.1\n",
        "#   criterion:\n",
        "#     distribution: categorical\n",
        "#     value:\n",
        "#       - ce\n",
        "#   optimizer:\n",
        "#     distribution: categorical\n",
        "#     values:\n",
        "#       - adam\n",
        "#       - SGD\n",
        "#       - RMSprop\n",
        "#   model_type:\n",
        "#     distribution: categorical\n",
        "#     values:\n",
        "#       - pretrained\n",
        "#       - scratch\n",
        "#   num_epochs:\n",
        "#     value:\n",
        "#       - 30\n",
        "#   scheduler_thresh:\n",
        "#     distribution: uniform\n",
        "#     min: 0.001\n",
        "#     max: 0.01\n",
        "#   scheduler_patience:\n",
        "#     distribution: int_uniform\n",
        "#     min: 2\n",
        "#     max: 10\n"
      ],
      "metadata": {
        "id": "tMbRxfDwZT8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2\n",
        "# After using the above config on wandb website, you will get a sweep id in return.\n",
        "# E.g. sweep id- dhruv_sri/wandb_demo/hbyp0tl8\n",
        "#\n",
        "# Add the following agent line in your code-\n",
        "# Use the generated sweep id in the below code\n",
        "\n",
        "wandb.agent(sweep_id=\"### FILL SWEEP ID HERE ###\", function=sweep_agent_manager, count=100)\n"
      ],
      "metadata": {
        "id": "8yEkgDnGZT2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3\n",
        "# Notice in above command we mentioned an argument named \"function\"\n",
        "# Wandb agents must trigger a function where they can initiate a socket to wandb and get a config.\n",
        "# So, we will use the following sweep_agent_manager function here-\n",
        "\n",
        "def sweep_agent_manager():\n",
        "    wandb.init()\n",
        "    config = dict(wandb.config)\n",
        "    run_name = f\"{config['model_type']}_{config['optimizer']}_{config['lr']}\"\n",
        "    wandb.run.name = run_name\n",
        "    trigger_training(config)\n"
      ],
      "metadata": {
        "id": "bqY9TPyJZTx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Done.\n",
        "# Now execute your training script on multiple machines.\n",
        "# Each run will request the config file from wandb and related experiments will be logged.\n",
        "# \n",
        "# NOTE!! wandb.log(data_to_log) must be present inside the code!! Else there is no meaning to sweep.\n"
      ],
      "metadata": {
        "id": "alHqJ4mQZTtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------ Ends ------------------------------"
      ],
      "metadata": {
        "id": "aNIxPqn_OGTU"
      }
    }
  ]
}